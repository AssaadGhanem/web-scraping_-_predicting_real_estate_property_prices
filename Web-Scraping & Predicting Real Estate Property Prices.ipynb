{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7b2bea42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\assaa\\anaconda3\\lib\\site-packages (4.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.0.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fcfe6217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\assaa\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from xgboost) (1.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\assaa\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6dcdd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import automated web-scraping modules\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# import time library for waiting\n",
    "import time\n",
    "\n",
    "# import numpy and pandas library for numerical data manipulation and exploratory analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import re for manipulating regular expressions\n",
    "import re\n",
    "\n",
    "# import matplotlib and seaborn for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc53de",
   "metadata": {},
   "source": [
    "<b>WEB-SCRAPING</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "26b970db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Install Chrome Driver Manager\n",
    "driver = webdriver.Chrome(service=ChromeService(executable_path=ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2fa5c65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\assaa\\AppData\\Local\\Temp\\ipykernel_25172\\1471295826.py:5: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(chrome_options=opts)\n"
     ]
    }
   ],
   "source": [
    "# Communicate your user-agent\n",
    "opts = Options()\n",
    "opts.add_argument\n",
    "(\"user-agent= Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\")\n",
    "driver = webdriver.Chrome(chrome_options=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6b433a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the first page that we need data from\n",
    "driver.get(\"https://www.superimmo.com/achat/ile-de-france/p/0\")\n",
    "\n",
    "# Take a break\n",
    "time.sleep(2)\n",
    "\n",
    "# Find the button of the pop-up window on the web page\n",
    "button = driver.find_element(By.XPATH,\"//*[@id='tarteaucitronPersonalize2']\")\n",
    "\n",
    "# Take a break\n",
    "time.sleep(2)\n",
    "\n",
    "# Click the button\n",
    "button.click()\n",
    "\n",
    "# Take a break\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a0c605fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over the real estate ads pages to scrape all the ad data necessary\n",
    "\n",
    "# Create the list that will hold all the ads scraped at the end of the process\n",
    "all_data_dict = []\n",
    "\n",
    "# Choose the number of pages to loop over - here we chose 499 pages\n",
    "for n in range(1,500):\n",
    "    \n",
    "    # Choose the number of ads per page to loop over - here we chose 10 pages\n",
    "    for m in range(1,11):\n",
    "        \n",
    "        # Open each page with the below command \n",
    "        driver.get(\"https://www.superimmo.com/achat/ile-de-france/p/{}\".format(n))\n",
    "        \n",
    "        # Take a break and stay ethical \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Create a dictonary for each page that includes all the features to scrape for each ad\n",
    "        data_dict = {}\n",
    "\n",
    "        # Scrape the property price of each ad\n",
    "        property_price = driver.find_element(by=By.XPATH,value=\n",
    "                                             \"//*[@id='pjax-container']/section/article[{}]/section/div[3]/p/a/b[1]\".format(m))\n",
    "\n",
    "        # Format the property price tightly\n",
    "        data_dict['price EUR'] = property_price.text\n",
    "        # Create conditional choices to avoid breaking the process flow\n",
    "        if data_dict['price EUR'] == '':\n",
    "            data_dict['price EUR'] = data_dict['price EUR']\n",
    "        elif isinstance(data_dict['price EUR'] , str) == True:\n",
    "            data_dict['price EUR'] = data_dict['price EUR']\n",
    "        else:\n",
    "            data_dict['price EUR'] = int(re.sub(r'[^0-9]', '', data_dict['price EUR']))\n",
    "        \n",
    "        # Scrape the property type, its size, its number of cells and its number of rooms\n",
    "        type_size_cells_rooms = driver.find_element(by=By.XPATH,value=\n",
    "                                                    \"//*[@id='pjax-container']/section/article[{}]/section/div[3]/p/a/b[2]\".format(m))\n",
    "\n",
    "        # Format the property type tightly\n",
    "        data_dict['type'] = type_size_cells_rooms.text.split(\" • \")\n",
    "        data_dict['type'] = data_dict['type'][0]\n",
    "\n",
    "        # Format the property size tightly\n",
    "        data_dict['size m²'] = type_size_cells_rooms.text.split(\" • \")\n",
    "        \n",
    "        # Create conditional choices to avoid breaking the process flow\n",
    "        if \" m²\" in data_dict['size m²'][1]:\n",
    "            data_dict['size m²'] = data_dict['size m²'][1]\n",
    "            data_dict['size m²'] = data_dict['size m²'].replace(\" m²\", \"\")\n",
    "            if \",\" in data_dict['size m²']:\n",
    "                data_dict['size m²'] = float(data_dict['size m²'].replace(\" m²\", \"\").replace(\",\",\".\"))\n",
    "            else:\n",
    "                data_dict['size m²'] = int(data_dict['size m²'].replace(\" \", \"\"))\n",
    "        else:\n",
    "            data_dict['size m²'] == None\n",
    "\n",
    "        # Format the number of cells of a property tightly\n",
    "        data_dict['cells'] = type_size_cells_rooms.text\n",
    "        \n",
    "        # Create conditional choices to avoid breaking the process flow\n",
    "        if \"pièces\" in data_dict['cells']:\n",
    "            sindex = re.search(r\"pièces\",data_dict['cells']).start()\n",
    "            data_dict['cells'] = data_dict['cells'][re.search(r\"pièces\",data_dict['cells']).start()-2:sindex+len(r\"pièces\")]\n",
    "            data_dict['cells'] = int(data_dict['cells'].replace(\" pièces\", \"\"))\n",
    "        elif \"pièce\" in data_dict['cells']:\n",
    "            sindex = re.search(r\"pièce\",data_dict['cells']).start()\n",
    "            data_dict['cells'] = data_dict['cells'][re.search(r\"pièce\",data_dict['cells']).start()-2:sindex+len(r\"pièce\")]\n",
    "            data_dict['cells'] = int(data_dict['cells'].replace(\" pièce\", \"\"))\n",
    "        else:\n",
    "            data_dict['cells'] = None\n",
    "            \n",
    "        # Format the number of rooms of a property tightly\n",
    "        data_dict['rooms'] = type_size_cells_rooms.text\n",
    "        \n",
    "        # Create conditional choices to avoid breaking the process flow\n",
    "        if \"chambres\" in data_dict[\"rooms\"]:\n",
    "            sindex = re.search(r\"chambres\",data_dict[\"rooms\"]).start()\n",
    "            data_dict[\"rooms\"] = data_dict[\"rooms\"][re.search(r\"chambres\",data_dict[\"rooms\"]).start()-2:sindex+len(r\"chambres\")]\n",
    "            data_dict[\"rooms\"] = int(data_dict[\"rooms\"].replace(\" chambres\", \"\"))\n",
    "        elif \"chambre\" in data_dict[\"rooms\"]:\n",
    "            sindex = re.search(r\"chambre\",data_dict[\"rooms\"]).start()\n",
    "            data_dict[\"rooms\"] = data_dict[\"rooms\"][re.search(r\"chambre\",data_dict[\"rooms\"]).start()-2:sindex+len(r\"chambre\")]\n",
    "            data_dict[\"rooms\"] = int(data_dict[\"rooms\"].replace(\" chambre\", \"\"))\n",
    "        else:\n",
    "            data_dict['rooms'] = None\n",
    "        \n",
    "        # Scrape the city and postal code\n",
    "        city_postcode = driver.find_element(by=By.XPATH,value=\n",
    "                                            \"//*[@id='pjax-container']/section/article[{}]/section/div[3]/b\".format(m))\n",
    "\n",
    "        # Format the city tightly\n",
    "        data_dict['city']= city_postcode.text\n",
    "        data_dict['city']= re.sub('[0-9]','', data_dict['city'])\n",
    "        data_dict['city']= re.sub(r'\\W+', '', data_dict['city'])\n",
    "        \n",
    "        # Create conditional choices to avoid breaking the process flow\n",
    "        if data_dict['city'] == \"Parisème\":\n",
    "            sindex = re.search(r\"Paris\",data_dict[\"city\"]).start()\n",
    "            data_dict['city'] = data_dict['city'][re.search(r\"Paris\",data_dict['city']).start():sindex+len(r\"Paris\")]\n",
    "        else:\n",
    "            data_dict['city']= data_dict['city']\n",
    "\n",
    "        # Format the postal code tightly\n",
    "        data_dict['postal_code']= city_postcode.text\n",
    "        data_dict['postal_code']= re.sub('[^0-9\\,]','', data_dict['postal_code'])\n",
    "        \n",
    "        # Create conditional choices to avoid breaking the process flow\n",
    "        if len(data_dict['postal_code']) == 7:\n",
    "            data_dict['postal_code'] = data_dict['postal_code'][2:]\n",
    "        elif len(data_dict['postal_code']) == 6:\n",
    "            data_dict['postal_code'] = data_dict['postal_code'][1:]\n",
    "        else:\n",
    "            data_dict['postal_code'] = data_dict['postal_code']\n",
    "            \n",
    "        # Take a break and stay ethical \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Append every 10 real estate ads scraped to the external list\n",
    "        all_data_dict.append(data_dict)\n",
    "        \n",
    "        # Create a dataframe out of the full list\n",
    "        df = pd.DataFrame.from_dict(all_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb16da8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the first 5 rows of the dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241161a0",
   "metadata": {},
   "source": [
    "<b>EXPLORATORY DATA ANALYSIS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataset\n",
    "print(\"The dataset is composed of\", df.shape[0], \"rows and\", df.shape[1], \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9a7d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the information of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affe4f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values per feature column with the data type of each column\n",
    "percent_missing = df.isnull().mean()\n",
    "missing_value_df = pd.DataFrame(percent_missing).reset_index() # convert to DataFrame\n",
    "missing_value_df = missing_value_df.rename(columns = {\"index\" : \"feature\", 0 : \"percent_missing\"}) # rename columns\n",
    "missing_value_df = missing_value_df.sort_values(by = ['percent_missing'], ascending = False) # sort the values\n",
    "data_types_df = pd.DataFrame(df.dtypes).reset_index().rename(columns = {\"index\" : \"feature\", 0 : \"data_type\"}) # rename columns\n",
    "missing_value_df = missing_value_df.merge(data_types_df, on = \"feature\") # join the dataframe with datatype\n",
    "missing_value_df.percent_missing = round(missing_value_df.percent_missing*100, 2) # format the percent_missing\n",
    "missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe948dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change some columns types to be able to manipulate the data\n",
    "df[\"size m²\"] = pd.to_numeric(df[\"size m²\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa95e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover the count of different values within each categorical column \n",
    "categorical_columns = list(df.select_dtypes(['object']).columns)\n",
    "cat_df = []\n",
    "for c in categorical_columns:\n",
    "    cat_df.append({\"categorical_feature\": c, \"number_categories\": len(df[c].value_counts(dropna = False))})\n",
    "pd.DataFrame(cat_df).sort_values(by = \"number_categories\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values of rooms column with \"0\"\n",
    "df[\"rooms\"] = df[\"rooms\"].fillna(0)\n",
    "\n",
    "# Fill null values of cells column with \"1\"\n",
    "df[\"cells\"] = df[\"cells\"].fillna(1)\n",
    "\n",
    "# Fill null values of size column with the average size of all apartments \n",
    "df[\"size m²\"] = df[\"size m²\"].fillna(df[\"size m²\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values one last time to make sure there are no missing data anymore\n",
    "percent_missing = df.isnull().mean()\n",
    "missing_value_df = pd.DataFrame(percent_missing).reset_index() # convert to DataFrame\n",
    "missing_value_df = missing_value_df.rename(columns = {\"index\" : \"feature\", 0 : \"percent_missing\"}) # rename columns\n",
    "missing_value_df = missing_value_df.sort_values(by = ['percent_missing'], ascending = False) # sort the values\n",
    "data_types_df = pd.DataFrame(df.dtypes).reset_index().rename(columns = {\"index\" : \"feature\", 0 : \"data_type\"}) # rename columns\n",
    "missing_value_df = missing_value_df.merge(data_types_df, on = \"feature\") # join the dataframe with datatype\n",
    "missing_value_df.percent_missing = round(missing_value_df.percent_missing*100, 2) # format the percent_missing\n",
    "missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check summary statistics of the numerical features of the dataset\n",
    "round(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify correlations in data\n",
    "corr = df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955dbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify patterns to look for correlations between features\n",
    "correlation_threshold = 0.3\n",
    "corr_matrix = df.corr().abs() # calculate the correlation matrix with \n",
    "high_corr_var = np.where(corr_matrix >= correlation_threshold) # identify variables that have correlations above defined threshold\n",
    "high_corr_var = [(corr_matrix.index[x], corr_matrix.columns[y], round(corr_matrix.iloc[x, y], 2))\n",
    "                     for x, y in zip(*high_corr_var) if x != y and x < y] # identify pairs of highly correlated variables\n",
    "if high_corr_var != []:\n",
    "    high_corr_var_df = pd.DataFrame(high_corr_var).rename(columns = {0: 'corr_feature',\n",
    "                                                                 1: 'drop_feature',\n",
    "                                                                 2: 'corrrelation_values'})\n",
    "    high_corr_var_df = high_corr_var_df.sort_values(by = 'corrrelation_values', ascending = False)\n",
    "else:\n",
    "    high_corr_var_df = print(\"there are no pairs of correlations with that threshold\")\n",
    "high_corr_var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d690a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify features with low variance\n",
    "std_df = pd.DataFrame(df.std()).rename(columns = {0: 'standard_deviation'})\n",
    "low_var_features = list(std_df[std_df['standard_deviation'] < 0.1].index)\n",
    "print(\"number of low variance features:\", len(low_var_features))\n",
    "print(\"low variance features:\", low_var_features)\n",
    "low_var_features\n",
    "\n",
    "# Below the threshold of 10%, there are no low-variance features in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99660183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the average count of real estate ads scraped per city\n",
    "print(\"Average count of real estate ads per city is:\", round(df.groupby('city')['city'].count().mean(),1), \"ads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee0830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the number of cities scraped\n",
    "print(\"The number of cities scraped is:\", df[\"city\"].nunique(), \"cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49297abe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the percentage representation of top 10 cities in terms of number of properties scraped\n",
    "round(df[\"city\"].value_counts(normalize = True)*100, 1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6ffb6",
   "metadata": {},
   "source": [
    "<b>FEATURE ENGINEERING</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6851fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy dataframe of df to avoid losing any progress\n",
    "df_eng = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d551f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check count of cell values\n",
    "df_eng[\"cells\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb37e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0 cell with 1 cell\n",
    "df_eng[\"cells\"] = df_eng[\"cells\"].replace(0.0, 1.0)\n",
    "df_eng[\"cells\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a price per sqm column\n",
    "df_eng[\"price/m²\"] = round(df_eng[\"price EUR\"] / df_eng[\"size m²\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop price EUR and size m² columns\n",
    "df_eng = df_eng.drop(columns = [\"price EUR\", \"size m²\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3d6c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count unique values of postal codes\n",
    "df_eng[\"postal_code\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86aefc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a district column to decrease the number of postal codes\n",
    "df_eng[\"district_code\"] = df_eng[\"postal_code\"].str[:2]\n",
    "df_eng[\"district_code\"] = df_eng[\"district_code\"].replace(\"\", None)\n",
    "df_eng[\"district_code\"] = df_eng[\"district_code\"].str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the city and postal code columns\n",
    "df_eng = df_eng.drop(columns = [\"city\", \"postal_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bceaf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count district code values\n",
    "df_eng[\"district_code\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff79ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in price/m²\n",
    "def find_outliers_IQR(df_eng):\n",
    "    q1=df_eng.quantile(0.25)\n",
    "    q3=df_eng.quantile(0.75)\n",
    "    IQR=q3-q1\n",
    "    outliers = df_eng[((df_eng<(q1-1.5*IQR)) | (df_eng>(q3+1.5*IQR)))]\n",
    "    return outliers\n",
    "price_per_sqm_outliers = find_outliers_IQR(df_eng[\"price/m²\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9602f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exclude outliers by limiting the visualized values\n",
    "y = df_eng[\"price/m²\"][df_eng[\"price/m²\"] < 50000]\n",
    "\n",
    "# Choose the figure size\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Choose the grid style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot the the price per square meter variations per district code\n",
    "box_plot = sns.boxplot(data=df_eng, x=df_eng[\"district_code\"].sort_values(ascending=True), y=y)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca6075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check unique types of properties\n",
    "df_eng[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Appartement neuf\" with \"Appartement\" because there is no need for this small complexity\n",
    "df_eng[\"type\"] = df_eng[\"type\"].replace(\"Appartement neuf\", \"Appartement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values from the dataset\n",
    "df_eng = df_eng.dropna()\n",
    "\n",
    "# Re-check the shape of the dataset\n",
    "print(\"The dataset is now composed of\", df_eng.shape[0], \"rows and\", df_eng.shape[1], \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834beb73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# One-hot encode the categorical variables: type and district_code\n",
    "for c in df_eng[[\"type\", \"district_code\"]]:\n",
    "    dummies = pd.get_dummies(df_eng[c], prefix=c)\n",
    "    df_eng = pd.concat([df_eng, dummies], axis=1)\n",
    "    df_eng.drop(c, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 10 rows of the final dataset for machine learning processing\n",
    "df_eng.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d898018",
   "metadata": {},
   "source": [
    "<b>MACHINE LEARNING MODEL</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee69c7",
   "metadata": {},
   "source": [
    "GRADIENT BOOSTING REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sci-kit Learn modules for processing, fitting and evaluation of the Gradient Boosting Regressor model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ac6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data points with Principal Component Analysis \n",
    "pca = PCA()\n",
    "pca = pca.fit(df_eng)\n",
    "dataPCA = pca.transform(df_eng)\n",
    "dataPCA = pd.DataFrame(dataPCA)\n",
    "\n",
    "# Rename the resulting columns of the transformation to their original names before the transformation \n",
    "dataPCA = dataPCA.rename(columns = {0:\"cells\", 1:\"rooms\", 2:\"price/m²\", 3:\"type_Appartement\", 4:\"type_Maison\", \n",
    "                                    5:\"district_code_75\", 6:\"district_code_77\", 7:\"district_code_78\", 8:\"district_code_91\", \n",
    "                                    9:\"district_code_92\", 10:\"district_code_93\", 11:\"district_code_94\", 12:\"district_code_95\"})\n",
    "\n",
    "# Take a look on the transformed dataset\n",
    "dataPCA.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify X as the features and y as the variable to predict\n",
    "X = dataPCA.drop(columns=[\"price/m²\"])\n",
    "y = dataPCA[\"price/m²\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y into train and test sets\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3, random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b817f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Gradient Boosting Regressor algorithm\n",
    "gbr = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31d43c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit the model on the train set\n",
    "gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c717508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the variable\n",
    "y_pred = gbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by comparing the r_squared score of the train set to the one of the test set\n",
    "print(r2_score(y_train,gbr.predict(X_train))*100)\n",
    "print(r2_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by checking its mean squared error\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve of the model by choosing different metrics for evaluation\n",
    "train_sizes, train_scores, test_scores = learning_curve(gbr, X, y, cv=10, scoring='r2', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60689fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the mean and the standard deviation of the training set \n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Identify the mean and the standard deviation of the test set \n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5655e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the figure size\n",
    "plt.subplots(1, figsize=(5,5))\n",
    "\n",
    "# Plot the trained data sizes on the x-axis and both the training mean and test mean on the y-axis\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Fill the sides of the curves with mean + or - the standard deviation of the dataset for both train and test sets\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Choose a title for the plot\n",
    "plt.title(\"Learning Curve\")\n",
    "\n",
    "# Choose labels and legend for the plot\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"R2 Score\"), plt.legend(loc=\"best\")\n",
    "\n",
    "# Specify the layout style\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75cd869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a feature importance function \n",
    "def feature_importance_plot(model, X_train, n):\n",
    "    \"\"\"Plots feature importance - this only works for Decision Tree based Models\"\"\"\n",
    "    plt.figure(figsize=(8, 5)) # set figure size\n",
    "    feat_importances = pd.Series(model.feature_importances_,\n",
    "                                 index = X_train.columns)\n",
    "    feat_importances.nlargest(n).plot(kind = 'bar')\n",
    "    plt.title(f\"Top {n} Features\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ab81b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the feature importance of the Gradient Boosting Regressor model\n",
    "feature_importance_plot(gbr, X_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb683085",
   "metadata": {},
   "source": [
    "RANDOM FOREST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Random Forest Regressor model\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data points with Principal Component Analysis \n",
    "pca = PCA()\n",
    "pca = pca.fit(df_eng)\n",
    "dataPCA = pca.transform(df_eng)\n",
    "dataPCA = pd.DataFrame(dataPCA)\n",
    "\n",
    "# Rename the resulting columns of the transformation to their original names before the transformation \n",
    "dataPCA = dataPCA.rename(columns = {0:\"cells\", 1:\"rooms\", 2:\"price/m²\", 3:\"type_Appartement\", 4:\"type_Maison\", \n",
    "                                    5:\"district_code_75\", 6:\"district_code_77\", 7:\"district_code_78\", 8:\"district_code_91\", \n",
    "                                    9:\"district_code_92\", 10:\"district_code_93\", 11:\"district_code_94\", 12:\"district_code_95\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8eae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify X as the features and y as the variable to predict\n",
    "X = dataPCA.drop(columns=[\"price/m²\"])\n",
    "y = dataPCA[\"price/m²\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y into train and test sets\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3, random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05181bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Random Forest Regressor algorithm\n",
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the train set\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the variable\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4acaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by comparing the r_squared score of the train set to the one of the test set\n",
    "print(r2_score(y_train,rf.predict(X_train))*100)\n",
    "print(r2_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by checking its mean squared error\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve of the model by choosing different metrics for evaluation\n",
    "train_sizes, train_scores, test_scores = learning_curve(rf, X, y, cv=10, scoring='r2', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the mean and the standard deviation of the training set \n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Identify the mean and the standard deviation of the test set \n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66a322",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Choose the figure size\n",
    "plt.subplots(1, figsize=(5,5))\n",
    "\n",
    "# Plot the trained data sizes on the x-axis and both the training mean and test mean on the y-axis\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Fill the sides of the curves with mean + or - the standard deviation of the dataset for both train and test sets\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Choose a title for the plot\n",
    "plt.title(\"Learning Curve\")\n",
    "\n",
    "# Choose labels and legend for the plot\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"R2 Score\"), plt.legend(loc=\"best\")\n",
    "\n",
    "# Specify the layout style\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5776abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importance of the Random Forest Regressor model\n",
    "feature_importance_plot(rf, X_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085b052",
   "metadata": {},
   "source": [
    "XGBOOST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the XGBoost Regressor model\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data points with Principal Component Analysis \n",
    "pca = PCA()\n",
    "pca = pca.fit(df_eng)\n",
    "dataPCA = pca.transform(df_eng)\n",
    "dataPCA = pd.DataFrame(dataPCA)\n",
    "dataPCA = dataPCA.rename(columns = {0:\"cells\", 1:\"rooms\", 2:\"price/m²\", 3:\"type_Appartement\", 4:\"type_Maison\", \n",
    "                                    5:\"district_code_75\", 6:\"district_code_77\", 7:\"district_code_78\", 8:\"district_code_91\", \n",
    "                                    9:\"district_code_92\", 10:\"district_code_93\", 11:\"district_code_94\", 12:\"district_code_95\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ffe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify X as the features and y as the variable to predict\n",
    "X = dataPCA.drop(columns=[\"price/m²\"])\n",
    "y = dataPCA[\"price/m²\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19839b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y into train and test sets\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3, random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the XGBoost Regressor algorithm\n",
    "xgb = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c45096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the train set\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the variable\n",
    "y_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by comparing the r_squared score of the train set to the one of the test set\n",
    "print(r2_score(y_train,xgb.predict(X_train))*100)\n",
    "print(r2_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9dc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by checking its mean squared error\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve of the model by choosing different metrics for evaluation\n",
    "train_sizes, train_scores, test_scores = learning_curve(xgb, X, y, cv=10, scoring='r2', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990312ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the mean and the standard deviation of the training set\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Identify the mean and the standard deviation of the test set \n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e63b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the figure size\n",
    "plt.subplots(1, figsize=(5,5))\n",
    "\n",
    "# Plot the trained data sizes on the x-axis and both the training mean and test mean on the y-axis\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Fill the sides of the curves with mean + or - the standard deviation of the dataset for both train and test sets\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Choose a title for the plot\n",
    "plt.title(\"Learning Curve\")\n",
    "\n",
    "# Choose labels and legend for the plot\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"R2 Score\"), plt.legend(loc=\"best\")\n",
    "\n",
    "# Specify the layout style\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ef7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importance of the XGBoost Regressor model\n",
    "feature_importance_plot(xgb, X_train, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
